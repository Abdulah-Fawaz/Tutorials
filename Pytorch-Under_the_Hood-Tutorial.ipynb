{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What exactly is PyTorch doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Source deep learning software framework developed by Facebook's AI research group.\n",
    "\n",
    "Based on the previously popular Torch framework that \n",
    "\n",
    "PyTorch: Written in Python, C, C++, CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of Talk:\n",
    "\n",
    " - Mathematics of Deep Learning / Backpropagation\n",
    " \n",
    " - Automatic Differentation, how and why?\n",
    " \n",
    " - How autodiff is implemented in PyTorch (vs others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![c](https://i.postimg.cc/4xjVQ04r/pytorch.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackPropogation:\n",
    "\n",
    "    \n",
    "Neural Networks are trained using **backpropagation**\n",
    " \n",
    "Backpropogation is essentially the chain rule of differentation in action.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "If $z = f(y)$ and $y = g(x)$:\n",
    "$$\n",
    "\\frac{dz}{dx}= \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "### How does BackPropogation work?\n",
    "\n",
    "Backpropogation is simply the application of the chain rule, and partial differentation, to function approximators that we call *Neural Networks*\n",
    "#### Worked Example\n",
    "\n",
    "Here is a reference image for a simple Neural Network layer.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png\" width=\"500\">\n",
    "Remember that this shows just one arbitrary layer, $j$.\n",
    "<br/><br/>\n",
    "There are the weights $w_{ij}$, the network input $net_j$ (which can be the weighted sum of $o_k$ previous neurons), and the output $o_j$.  \n",
    "Our aim is to minimise some error, $E$, which in this case we take to be a supervised task and thus given by \n",
    "$E = L(y_{true}, y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for the change in the Error with respect to changes in individual weights alone, $$\\frac{\\partial E}{\\partial w_{ij}}$$ So we use the chain rule twice:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial o_j} \\frac{\\partial o_j}{\\partial w_{ij}}\n",
    "= \\frac{\\partial E}{\\partial o_j}\\frac{\\partial o_j}{\\partial \\text{net}_j}\\frac{\\partial \\text{net}_j}{\\partial w_{ij}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b> Weight Formula: </b>$$\n",
    "\\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial o_j}\\frac{\\partial o_j}{\\partial \\text{net}_j}\\frac{\\partial \\text{net}_j}{\\partial w_{ij}}$$\n",
    "</div>\n",
    "\n",
    "### Breaking down these terms\n",
    "\n",
    "#### The net-weight term\n",
    "\n",
    "The third term in the above is the easiest to understand.\n",
    "$$\\frac{\\partial \\text{net}_j}{\\partial w_{ij}}$$\n",
    "Since net$_j$ is in fact defined as the sum of the previous outputs $o_k$ weighted by $w_{ij}$ so the only non vanishing term is the $w_{ij}$ term. Formally: <br></br>\n",
    "\n",
    "$$\\frac{\\partial \\text{net}_j}{\\partial w_{ij}} = \\frac{\\partial \\text{ }}{\\partial w_{ij}} \\big(\\sum_{k=1}^{n} w_{kj} o_k \\big) = o_i $$\n",
    "\n",
    "In plain terms, how does the network input (which is a weighted sum of previous outputs) change with one of the weights? It changes by an amount equal to the output being weighted. (If the layer $j$ is the input layer then instead of a previous output its an input.)\n",
    "\n",
    "#### The Activation Function Term\n",
    "Also easy to understand is $$\\frac{\\partial o_j}{\\partial \\text{net}_j}$$ \n",
    "since $o_j$ is directly related to $\\text{net}_j$ through the activation function $\\psi$\n",
    "$$ \\frac{\\partial o_j}{\\partial \\text{net}_j} = \\frac{\\partial \\psi(\\text{net}_j)}{\\partial \\text{net}_j}$$\n",
    "\n",
    "This is simply the derivative of the activation function. This is why it is often stated that the activation function should be a differentiable function. In fact, often the activation functions used have rather convenient forms for their derivates. An example is the logistics function <br></br>\n",
    "\n",
    "$$ \\psi(z) = \\frac{1}{1+e^{-z}}$$\n",
    "who's derivative is a lovely\n",
    "$$\\psi'(z) = \\psi(z)(1-\\psi(z))$$\n",
    "\n",
    "\n",
    "It's interesting to note that theres actually some flexibility here. Mathematically ReLU() is not a differentiable function but in practical terms its derivative can actually be very easily defined. \n",
    "ReLU is defined as $\\psi(z) = max(0,z)$ which has derivative 0 or 1 for negative, positive $z$ respectively. It is not defined for the case where $z=0$ so it is common to set it to some value, usually either $0, 0.5$ or $1$  PyTorch (**I THINK**) sets it to 0.\n",
    "\n",
    "An alternative approach is to approximate it with the differentiable function $f(x) = log(1+e^x)$ whos derivative is just the logistics function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Error Term\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial o_j}\n",
    "$$\n",
    "The first term is the difficult one. It is sometimes referred to as the *error* but I will avoid this due to potential confusion.\n",
    "\n",
    "##### If j is the output layer\n",
    "\n",
    "In the case that its the output layer, then it becomes fairly simple and depends on the construction of the problem and the exist error function. If we imagine a supervised learning problen, the output $o_j = y$ and we need to differentiate the loss function.\n",
    "With the L1 loss: <br></br>\n",
    "$$\\frac{\\partial E}{\\partial o_j}= \\frac{\\partial |y_{true} - y|}{\\partial y} = \\pm 1$$\n",
    "depending on whether or not $y$ is larger than $y_{true}$. \n",
    "The gradient is the same even for both very small and very large errors. This makes it harder to converge, but also more resistant to outliers. \n",
    "\n",
    "With the L2 loss the gradient is analytic: \n",
    "$$\\frac{\\partial E}{\\partial o_j}= \\frac{\\partial  \\frac{1}{2} (y_{true} - y)^2}{\\partial y} = (y_{true} - y)$$\n",
    "It can blow up for outliers, but will scale as the error increases, yielding much better convergence.\n",
    "\n",
    "##### If j is not an output layer\n",
    "$$\\frac{\\partial E}{\\partial o_j}$$\n",
    "\n",
    "One cannot arbitrarily calculate this, but **can** use the errors of subsequent layers (closer to the output). This is where backpropagation gets its name.\n",
    "\n",
    "The precise reason that it is difficult to evaluate the above term is because $E$ is dependent on $o_j$ in a non-trivial way depending on how the neural network propagates the value of $o_j$. We can resolve this using *total derivates*. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b> Total Derivative of Multivariable Function: </b>\n",
    "    \n",
    "For function $f\\big{(}g(x), y(x), z(x)\\big{)}$ one can take total derivates to find $\\large\\frac{df}{dx}$:\n",
    "\n",
    "$$\\frac{df}{dx} = \\frac{\\partial f}{\\partial g} \\frac{\\partial g}{\\partial x} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial x}+\\frac{\\partial f}{\\partial z} \\frac{\\partial z}{\\partial x}$$\n",
    "</div>\n",
    "\n",
    "We can think of the Error as an implicit function of $o_j$. Let the loss be a function of every single neuron that receives an input from $o_j$, i.e $L(u,v,w,x,y,z)$.\n",
    "\n",
    "We can therefore define $\\large\\frac{\\partial E}{\\partial o_j}$ recursively by calculating the gradients of the neurons in the following layer.\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial o_j}= \\sum_{k=1}^K \\big{(} \\frac{\\partial E}{\\partial o_k}\\frac{\\partial o_k}{\\partial o_j} \\big{)}\n",
    "$$where we define $K$ neurons in layer $k$ (i.e the layer after $j$).\n",
    "\n",
    "#### Lemma: Convolutional Layers:\n",
    "\n",
    "Convolutional layers have a nice interesting way of backpropagating their errors.\n",
    "\n",
    "By convolution.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*5QPqJEmBqotN4Dab2TWqFQ.jpeg\" width=\"500\">\n",
    "\n",
    "\n",
    "Using partial derivatives, one can find the gradient as:\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial F_{11}} = \\frac{\\partial E}{\\partial O_{11}} \\frac{\\partial O_{11}}{\\partial F_{11}} +\\frac{\\partial E}{\\partial O_{12}} \\frac{\\partial O_{12}}{\\partial F_{11}} +\\frac{\\partial E}{\\partial O_{21}} \\frac{\\partial O_{21}}{\\partial F_{11}} + \\frac{\\partial E}{\\partial O_{22}} \\frac{\\partial O_{22}}{\\partial F_{11}} \\text{, etc}$$ \n",
    "$\\frac{\\partial O_{11}}{\\partial F_{11}} = X_{11} \\ldots$ so error is in fact:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*auj7ULC2kRCa99_6u1QSNA.jpeg\" width=\"550\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and Backward Passes\n",
    "\n",
    "Backpropagation is the backwards *propogation* of the $\\large\\frac{\\partial E}{\\partial o_j}$ error term for each layer. This is the **backward pass**. However, the actual values needed to calculate the gradients can only be calculated during a **forward pass** of the neural network. \n",
    "\n",
    "So a standard backpropogation will begin with a forward pass that utilises the inputs, weights and activation functions, and the backward pass will use all the outputs and activations to calculate the errors. \n",
    "\n",
    "**Note that all of the outputs are needed from the forward phase before the backward phase can begin.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable Programming with Automatic Differentiation:\n",
    "\n",
    "Differentiable programming is a programming paradigm which allows for rapid and efficient calculation of gradients of arbitrary functions and has played an enormous, albeit rather unseen, part in the growth and development of machine learning and, earlier, numerical optimization. \n",
    "\n",
    "The main method used - which is what will be discussed here - is **automatic differentiation**.\n",
    "\n",
    "**What is the derivative of $f(x) = \\mathbf{\\text{cos}(x)}$?**<br></br>\n",
    "$f'(x) = - \\text{sin}(x)$\n",
    "\n",
    "What about the derivative of an arbitrary function $f(x)$ at some point $x=a$?<br></br>\n",
    "HARDER THAN YOU THINK!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/AutomaticDifferentiationNutshell.png/1920px-AutomaticDifferentiationNutshell.png\" width=\"550\">\n",
    "\n",
    "**Option 1: Symbolic Differentiation**\n",
    "\n",
    "Inefficient code, difficult to convert entire program into single symbolic expression. Eg: *Mathematica*\n",
    "\n",
    "**Option 2: Numerical Differentation**\n",
    "\n",
    "Rounding errors creep in from discretization. \n",
    "\n",
    "\n",
    "For example, consider the method of *divided differences*, for the *simple* function $f(x) = x^3$\n",
    "\n",
    "$$\\text{error in estimate} = \\frac{f(x+h) - f(x)}{h} - 3x^2, \\text{for arbitrarily small h}$$\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/AbsoluteErrorNumericalDifferentiationExample.png/300px-AbsoluteErrorNumericalDifferentiationExample.png\" width=\"550\">\n",
    "\n",
    "The graph above shows that choice of h is difficult even for a very simple function. \n",
    "\n",
    "Both find it difficult to calculate higher derivatives. Both are **VERY BAD** at calculating partial derivates of functions of *many, many* arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution: Automatic Differentiation**\n",
    "\n",
    "Automatic differentation allows for a systematic way to find the gradient of a function.\n",
    "The best way to understand automatic differentation is through a worked example. <br></br>\n",
    "Consider the function $$f(x_1, x_2) = x_1 x_2 + \\text{sin} x_1$$\n",
    "\n",
    "How does one evaluate this function? Use *sub expressions*, $w$\n",
    "\n",
    "$$\\begin{align}\n",
    "z &= f(x_1,x_2) \\\\ &= x_1 x_2 + \\text{sin} (x_1) \\\\&= w_1 w_2 + \\text{sin} (w_1),  \\quad\\leftarrow w_1 = x_1, w_2 = x_2 \\\\&= w_3 + w_4, \\quad \\leftarrow w_3 = w_1 w_2, w_4 = \\text{sin} (w_1) \\\\ &= w_5 \\quad \\leftarrow w_5 = w_3 + w_4\\end{align}$$\n",
    "\n",
    "we can use this to create a *dual program* to find the derivate with respect to $x_2$.\n",
    "\n",
    "$$\\begin{align}\n",
    "&w_1 = x_1 \\quad \\quad \\quad \\quad \\dot{w_1} = 0 \\ (seed)\\\\\n",
    "&w_2 = x_2 \\quad \\quad \\quad \\quad \\dot{w_2} = 1 \\ (seed)\\\\\n",
    "&w_3 = w_1 \\cdot w_2 \\quad \\quad \\dot{w_3} = \\dot{w_1}w_2 +\\dot{w_2} w_1 = 0 \\cdot x_2 + 1 \\cdot x_1\\\\\n",
    "&w_4 = \\text{sin}(w_1) \\quad \\quad \\dot{w_4} = \\text{cos}(w_1) \\dot{w_1} = 0 \\cdot \\text{cos}(x_1)\\\\\n",
    "&w_5 = w_3 + w_4 \\quad \\quad \\dot{w_5} = \\dot{w_3}w_4 +\\dot{w_4} w_3 = x_1+0 = x_1\n",
    "\\end{align}$$\n",
    "reveals that $\\large{\\frac{\\partial f}{\\partial x_2}} = x_1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is well represented using computational graphs. The same computational graph that calculates a function can be adapted to also calculate its derivative.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/ForwardAccumulationAutomaticDifferentiation.png/450px-ForwardAccumulationAutomaticDifferentiation.png\" width=\"550\">\n",
    "There are of course, reverse computational graphs too, for the backward pass. The reverse path calculates $\\large\\frac{\\partial f}{\\partial w_i}$$\\equiv \\bar{w_i}$ \n",
    "$$\\begin{align}\n",
    "&w_1 = x_1 \\\\\n",
    "&w_2 = x_2 \\\\\n",
    "&w_3 = w_1 \\\\\n",
    "&w_4 = \\text{sin}(w_1) \\\\\n",
    "&w_5 = w_3 + w_4 \n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "$$\\begin{align}\n",
    "&\\bar{w_5} = 1 (seed)\\\\\n",
    "&\\bar{w_4} = \\bar{w_5} \\quad \\text{since } \\bar{w_4} = \\bar{w_5} \\cdot \\frac{\\partial w_5}{\\partial w_4}\\\\\n",
    "&\\bar{w_3} = \\bar{w_5}\\\\\n",
    "&\\bar{w_2} = \\bar{w_3} \\cdot w_1\\\\\n",
    "&\\bar{w_1} = \\bar{w_3} \\cdot w_2 + \\bar{w_4}\\cdot \\text{cos}(w_1)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/ReverseaccumulationAD.png/450px-ReverseaccumulationAD.png\" width=\"550\">\n",
    "\n",
    "\n",
    "Backwards and forwards computation of gradients are roughly equivalent, but backwards is more memory intensive as it requires access to the intermediate variables $w_i$. \n",
    "\n",
    "\n",
    "**Constructing and storing computational graphs is the main role of deep learning frameworks. It defines what functions they can find gradients for, and how. In PyTorch, this is done in C++ for efficiency.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static vs Dynamic Graphs\n",
    "\n",
    "## Static\n",
    "\n",
    "At this point, I have explained the method from a static approach. We took the entire function and then derived an expression for its derivative. This method is known as *source code transformation* and takes place at compiler level. It is actually surprisingly efficient, but at the same time very inflexible. It was a huge issue for TensorFlow that they couldnt build graphs dynamically and therefore handle different length sentences in NLP applications because of it but they did eventually find a workaround to make their graphs dynamic.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/SourceTransformationAutomaticDifferentiation.png/1920px-SourceTransformationAutomaticDifferentiation.png\" width=\"550\">\n",
    "\n",
    "## Dynamic\n",
    "\n",
    "The alternative is to have dynamic graphs. What this means is that the graph is computed at run-time, giving much greater flexibility. It does this by combining *gradient-enabled tensors* as *variables* alongside functions as operators.\n",
    "Instead the user defines the graph when they define variables. So when you define a variable in PyTorch, there is an unseen boolean *requires_grad* attached (default True). If True, PyTorch will track the operation history and the autograd class will include this variable in the backwards graph and calculate its gradients. \n",
    "\n",
    "### Differentiating with Dual Numbers\n",
    "\n",
    "How does autograd do derivatives so dynamically? It uses Dual Numbers.\n",
    "\n",
    "Dual numbers extend the field of real numbers similar to complex numbers.\n",
    "\n",
    "**Complex Numbers**:  $z \\rightarrow a + bi, \\quad i^2 = -1$\n",
    "\n",
    "**Dual Numbers**:  $z \\rightarrow a + b\\epsilon \\quad \\epsilon^2 = 0$\n",
    "\n",
    "Dual numbers gives you the derivatives of functions by evaluating them.\n",
    "\n",
    "Given any polynomial $$P(x) = p_0 + p_1 x + p_2 x^2 \\ldots$$\n",
    "We can calculate the derivative by extending $x$ to include dual numbers.\n",
    "\n",
    "$$\\begin{align}P(a+b\\epsilon) &= p_0 + p_1 (a+b\\epsilon) + p_2 (a+b\\epsilon)^2 + \\ldots...+ p_n(a+b\\epsilon)^n\\\\\n",
    "= p_0 + &p_1 a + p_2 a^2 +\\ldots +p_n a^n \\\\\n",
    "&= p_1 b\\epsilon + 2p_2 ba\\epsilon + \\ldots + n p_n a^{n-1} b \\epsilon\\\\\n",
    "&= P(a) + b\\epsilon P'(a)\n",
    "\\end{align}$$\n",
    "\n",
    "$P'(a)$ can be read off in the dual component of the result. By using Taylor series expansions, we can apply this to any real valued function (and indeed this is the usual way that functions like sin and log are computed).\n",
    "\n",
    "This changed arithmetic requires flexibility in mathematical operations and objects defined for real numbers. This is achieved using *operator overloading*. Operator overleading is where the definition of operations and even data types can be adapted. Not all programming languages support this. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/OperatorOverloadingAutomaticDifferentiation.png/450px-OperatorOverloadingAutomaticDifferentiation.png\" width=\"550\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![c](https://i.postimg.cc/4xjVQ04r/pytorch.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "There is a looooot more to talk about, but I haven't had the time. \n",
    "\n",
    " - Its important to remember the maths behind it all, even if its mostly just the chain rule\n",
    " - Remember that you defining the computational graph, this gives you flexibility. \n",
    " - This is a changing field. Julia, for example, takes the syntax itself as the graph. One day, symbolic differentiation may render automatic differentiation mute.\n",
    " \n",
    " \n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
